{
  "framework": "NIST AI RMF 1.0",
  "version": "1.0",
  "description": "NIST AI Risk Management Framework â€” Functions and categories relevant to AI agent governance",
  "last_updated": "2026-02-15",
  "controls": [
    {
      "id": "GOVERN-1.1",
      "category": "Govern",
      "title": "Legal and Regulatory Requirements",
      "description": "Legal and regulatory requirements involving AI are understood, managed, and documented.",
      "agent_relevance": "high",
      "policy_mappings": [
        {
          "policy_pattern": "any_active_policy",
          "tool_patterns": ["*"],
          "coverage": "partial",
          "rationale": "Documented guardrail policies demonstrate awareness of requirements for AI agent operations"
        }
      ],
      "evidence_queries": {
        "description": "Policy documentation and compliance mapping artifacts"
      },
      "gap_recommendations": [
        "Document which regulations apply to your agent operations",
        "Map guardrail policies to specific regulatory requirements",
        "Use the Compliance Kit to generate framework-specific reports"
      ]
    },
    {
      "id": "GOVERN-1.5",
      "category": "Govern",
      "title": "Risk Tolerances",
      "description": "Ongoing monitoring and periodic review of the risk management process and its outcomes are planned and organizational risk tolerances are determined and documented.",
      "agent_relevance": "critical",
      "policy_mappings": [
        {
          "policy_pattern": "risk_threshold",
          "tool_patterns": ["*"],
          "coverage": "full",
          "rationale": "Risk threshold policies codify and enforce organizational risk tolerances for agent actions"
        },
        {
          "policy_pattern": "block",
          "tool_patterns": ["*"],
          "coverage": "partial",
          "rationale": "Block policies define absolute risk boundaries that agents cannot cross"
        }
      ],
      "evidence_queries": {
        "description": "Risk threshold configurations and enforcement history"
      },
      "gap_recommendations": [
        "Define explicit risk tolerances for agent operations",
        "Add risk_threshold policies with documented thresholds",
        "Review and adjust risk tolerances quarterly"
      ]
    },
    {
      "id": "GOVERN-4.1",
      "category": "Govern",
      "title": "Organizational Practices for AI Transparency",
      "description": "Organizational practices and norms that foster a critical thinking and safety-first mindset are understood and implemented.",
      "agent_relevance": "high",
      "policy_mappings": [
        {
          "policy_pattern": "require_approval",
          "tool_patterns": ["*"],
          "coverage": "partial",
          "rationale": "Human-in-the-loop approval gates demonstrate safety-first approach to agent operations"
        }
      ],
      "evidence_queries": {
        "description": "Approval workflow usage and human override statistics"
      },
      "gap_recommendations": [
        "Implement human-in-the-loop for high-risk agent actions",
        "Document agent capability boundaries and limitations",
        "Publish agent transparency reports for stakeholders"
      ]
    },
    {
      "id": "MAP-1.5",
      "category": "Map",
      "title": "Impacts to Individuals and Communities",
      "description": "Potential impacts to individuals, groups, communities, organizations, and society are identified and documented.",
      "agent_relevance": "high",
      "policy_mappings": [
        {
          "policy_pattern": "require_approval",
          "tool_patterns": ["message.send", "email.send", "social.post", "support.reply"],
          "coverage": "full",
          "rationale": "Approval gates on external communications ensure human review of potentially impactful agent outputs"
        }
      ],
      "evidence_queries": {
        "description": "External communication approval rates and denial reasons"
      },
      "gap_recommendations": [
        "Gate all customer-facing and public agent communications behind approval",
        "Document potential impact categories for agent operations",
        "Review denied communications to identify recurring impact concerns"
      ]
    },
    {
      "id": "MEASURE-2.6",
      "category": "Measure",
      "title": "AI System Performance Monitoring",
      "description": "The AI system is evaluated regularly for safety risks, and responses are documented and monitored.",
      "agent_relevance": "critical",
      "policy_mappings": [
        {
          "policy_pattern": "rate_limit",
          "tool_patterns": ["*"],
          "coverage": "full",
          "rationale": "Rate limiting monitors agent activity patterns for anomalous behavior"
        },
        {
          "policy_pattern": "any_active_policy",
          "tool_patterns": ["*"],
          "coverage": "partial",
          "rationale": "Continuous policy evaluation provides ongoing safety monitoring"
        }
      ],
      "evidence_queries": {
        "description": "Guard evaluation counts, rate limit triggers, and anomaly signals"
      },
      "gap_recommendations": [
        "Enable continuous guard policy evaluation on all agent actions",
        "Add rate limiting to detect abnormal activity patterns",
        "Enable DashClaw signal detection for safety monitoring",
        "Review guard logs and signals at least monthly"
      ]
    },
    {
      "id": "MEASURE-2.9",
      "category": "Measure",
      "title": "AI Model Security",
      "description": "The AI system is evaluated for security risks, including adversarial attacks and data poisoning.",
      "agent_relevance": "high",
      "policy_mappings": [
        {
          "policy_pattern": "block",
          "tool_patterns": ["exec.*", "shell.*", "code.eval"],
          "coverage": "full",
          "rationale": "Blocking code execution prevents prompt injection from escalating to system compromise"
        }
      ],
      "evidence_queries": {
        "description": "Blocked execution attempts that may indicate adversarial attacks"
      },
      "gap_recommendations": [
        "Block all arbitrary code execution by agents",
        "Implement input validation on agent tool parameters",
        "Monitor for prompt injection patterns in agent inputs",
        "Use DashClaw semantic_check policies for content safety"
      ]
    },
    {
      "id": "MANAGE-2.1",
      "category": "Manage",
      "title": "Risk Response and Recovery",
      "description": "Resources required to manage AI risks are taken into account, along with viable non-AI alternatives.",
      "agent_relevance": "high",
      "policy_mappings": [
        {
          "policy_pattern": "block",
          "tool_patterns": ["*"],
          "coverage": "partial",
          "rationale": "Block policies provide immediate risk response by halting dangerous agent actions"
        },
        {
          "policy_pattern": "require_approval",
          "tool_patterns": ["*"],
          "coverage": "partial",
          "rationale": "Approval gates enable human fallback for agent decisions"
        }
      ],
      "evidence_queries": {
        "description": "Policy override and human fallback usage statistics"
      },
      "gap_recommendations": [
        "Document human fallback procedures for each agent capability",
        "Implement emergency agent shutdown procedures",
        "Define escalation paths for agent risk events"
      ]
    },
    {
      "id": "MANAGE-4.1",
      "category": "Manage",
      "title": "Post-Deployment Monitoring",
      "description": "Post-deployment AI system monitoring plans are implemented, including mechanisms for capturing and evaluating input from users.",
      "agent_relevance": "critical",
      "policy_mappings": [
        {
          "policy_pattern": "any_active_policy",
          "tool_patterns": ["*"],
          "coverage": "partial",
          "rationale": "Active guard policies provide continuous post-deployment monitoring of agent behavior"
        }
      ],
      "evidence_queries": {
        "description": "Ongoing guard evaluation metrics and user feedback logs"
      },
      "gap_recommendations": [
        "Enable continuous guard policy evaluation in production",
        "Configure automated alerts for policy violations",
        "Collect and review agent interaction feedback from users",
        "Generate periodic compliance reports for stakeholder review"
      ]
    }
  ]
}
