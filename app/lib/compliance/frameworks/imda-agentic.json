{
  "framework": "Singapore IMDA Model Framework for Agentic AI",
  "version": "January 2026",
  "description": "Singapore Infocomm Media Development Authority framework for governance of agentic AI systems, focused on multi-agent operations and autonomous decision-making",
  "last_updated": "2026-02-15",
  "controls": [
    {
      "id": "IMDA-1.1",
      "category": "Human Oversight",
      "title": "Human-in-the-Loop for High-Risk Actions",
      "description": "Organizations should implement human oversight mechanisms for agentic AI systems, particularly for high-risk actions with significant real-world consequences.",
      "agent_relevance": "critical",
      "policy_mappings": [
        {
          "policy_pattern": "require_approval",
          "tool_patterns": ["*"],
          "coverage": "full",
          "rationale": "Approval gates enforce human-in-the-loop oversight for agent actions"
        }
      ],
      "evidence_queries": {
        "description": "Approval workflow usage: total requests, approval rates, response times"
      },
      "gap_recommendations": [
        "Implement approval requirements for high-risk agent actions",
        "Define clear criteria for what constitutes 'high-risk'",
        "Track approval response times to ensure timely oversight"
      ]
    },
    {
      "id": "IMDA-1.2",
      "category": "Human Oversight",
      "title": "Kill Switch and Override Mechanisms",
      "description": "Agentic AI systems should include mechanisms to immediately halt operations and override autonomous decisions when necessary.",
      "agent_relevance": "critical",
      "policy_mappings": [
        {
          "policy_pattern": "block",
          "tool_patterns": ["*"],
          "coverage": "partial",
          "rationale": "Block policies can serve as kill switches for specific agent capabilities"
        }
      ],
      "evidence_queries": {
        "description": "Policy disable/enable history and emergency override events"
      },
      "gap_recommendations": [
        "Implement emergency agent shutdown procedure",
        "Ensure all policies can be toggled in real-time (DashClaw provides this)",
        "Document and test kill switch procedures quarterly"
      ]
    },
    {
      "id": "IMDA-2.1",
      "category": "Transparency and Explainability",
      "title": "Agent Decision Audit Trail",
      "description": "Agentic AI systems should maintain comprehensive audit trails of autonomous decisions, including reasoning chains and tool invocations.",
      "agent_relevance": "critical",
      "policy_mappings": [
        {
          "policy_pattern": "any_active_policy",
          "tool_patterns": ["*"],
          "coverage": "partial",
          "rationale": "Guard decision logs provide audit trails of which actions were allowed, blocked, or required approval"
        }
      ],
      "evidence_queries": {
        "description": "Guard decision logs with timestamps, actions, decisions, and reasons"
      },
      "gap_recommendations": [
        "Ensure all agent tool invocations are logged with decision rationale",
        "Retain audit trails for a minimum of 12 months",
        "Enable DashClaw decision recording features for full governance coverage",
        "Generate periodic audit reports from decision logs"
      ]
    },
    {
      "id": "IMDA-3.1",
      "category": "Boundary and Scope Controls",
      "title": "Agent Capability Boundaries",
      "description": "Organizations should define and enforce clear boundaries on what agentic AI systems can and cannot do, limiting capabilities to the minimum necessary for the intended purpose.",
      "agent_relevance": "critical",
      "policy_mappings": [
        {
          "policy_pattern": "block",
          "tool_patterns": ["exec.*", "shell.*", "code.eval", "code.run"],
          "coverage": "full",
          "rationale": "Blocking arbitrary execution restricts agent capabilities to explicitly permitted tools"
        },
        {
          "policy_pattern": "allowlist",
          "tool_patterns": ["*"],
          "coverage": "full",
          "rationale": "Tool allowlisting implements the principle of least privilege for agent capabilities"
        }
      ],
      "evidence_queries": {
        "description": "Total blocked actions and allowlist configurations"
      },
      "gap_recommendations": [
        "Define explicit capability boundaries for each agent",
        "Block all tools not explicitly required for agent's purpose",
        "Use allowlisting over blocklisting where possible",
        "Review and minimize agent capabilities quarterly"
      ]
    },
    {
      "id": "IMDA-3.2",
      "category": "Boundary and Scope Controls",
      "title": "Inter-Agent Communication Controls",
      "description": "When multiple agents operate together, organizations should implement controls on inter-agent communication to prevent unauthorized delegation or escalation.",
      "agent_relevance": "high",
      "policy_mappings": [
        {
          "policy_pattern": "require_approval",
          "tool_patterns": ["message.send", "slack.send", "discord.send"],
          "coverage": "partial",
          "rationale": "Approval gates on messaging can control inter-agent communication when agents communicate via messaging tools"
        }
      ],
      "evidence_queries": {
        "description": "Inter-agent communication logs and approval records"
      },
      "gap_recommendations": [
        "Define allowed inter-agent communication patterns",
        "Implement access controls for agent-to-agent tool delegation",
        "Monitor for unauthorized agent task delegation or escalation",
        "Use separate API keys per agent for identity tracking"
      ]
    },
    {
      "id": "IMDA-4.1",
      "category": "Safety and Security",
      "title": "Adversarial Robustness",
      "description": "Agentic AI systems should be resilient against adversarial inputs including prompt injection, jailbreaking, and social engineering attacks.",
      "agent_relevance": "critical",
      "policy_mappings": [
        {
          "policy_pattern": "block",
          "tool_patterns": ["exec.*", "shell.*", "code.eval"],
          "coverage": "full",
          "rationale": "Blocking code execution prevents prompt injection from escalating to system compromise"
        }
      ],
      "evidence_queries": {
        "description": "Blocked execution attempts that may indicate adversarial inputs"
      },
      "gap_recommendations": [
        "Block all arbitrary code execution by agents",
        "Implement input validation on tool parameters",
        "Use DashClaw semantic_check policies for content safety",
        "Monitor for prompt injection patterns in agent inputs"
      ]
    },
    {
      "id": "IMDA-4.2",
      "category": "Safety and Security",
      "title": "External Action Safety",
      "description": "Agentic AI systems that take real-world actions (sending emails, making payments, modifying data) should implement proportional safety controls.",
      "agent_relevance": "critical",
      "policy_mappings": [
        {
          "policy_pattern": "require_approval",
          "tool_patterns": ["email.send", "payment.*", "billing.*", "deploy.*"],
          "coverage": "full",
          "rationale": "Approval gates on external actions ensure human oversight proportional to action severity"
        },
        {
          "policy_pattern": "block",
          "tool_patterns": ["fs.delete", "db.drop", "db.truncate"],
          "coverage": "full",
          "rationale": "Blocking destructive operations prevents irreversible damage from agent errors"
        }
      ],
      "evidence_queries": {
        "description": "External action approval rates and blocked destructive operations"
      },
      "gap_recommendations": [
        "Gate all external actions behind approval",
        "Block irreversible destructive operations",
        "Implement tiered controls proportional to action severity",
        "Use DashClaw dry-run mode to test policies against historical data"
      ]
    },
    {
      "id": "IMDA-5.1",
      "category": "Accountability",
      "title": "Operator Accountability",
      "description": "Organizations deploying agentic AI should establish clear accountability for agent actions, including designated responsible persons and documented governance procedures.",
      "agent_relevance": "high",
      "policy_mappings": [
        {
          "policy_pattern": "any_active_policy",
          "tool_patterns": ["*"],
          "coverage": "partial",
          "rationale": "Documented guardrail policies establish organizational accountability for agent behavior"
        }
      ],
      "evidence_queries": {
        "description": "Policy documentation, compliance reports, and governance artifacts"
      },
      "gap_recommendations": [
        "Designate a responsible person for agent governance",
        "Document agent governance procedures",
        "Generate and retain compliance reports quarterly",
        "Establish incident response procedures for agent failures"
      ]
    }
  ]
}
